
xgboost概念 参考：https://www.cnblogs.com/wj-1314/p/9402324.html

xgboost是在GBDT（gbdt全称梯度下降树）的基础上进行改进的，也是Boosting算法的其中一种，就是将弱学习器集成一起形成强学习器，所用到的树模型则是CART回归树模型。

xgboost算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，
其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，
在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。


GBDT
      gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。
      对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。

      弱分类器一般会选择为CART TREE（也就是分类回归树）。由于上述高偏差和简单的要求 每个分类回归树的深度不会很深。
      最终的总分类器 是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。
      
    GBDT（Gradient Boosting Decision Tree）是Boosting算法的一种，每轮迭代是在上一轮弱学习器的损失函数梯度下降的方向上，训练产生新的弱学习器，所有弱学习器加权组合得到最终的强学习器。
    
    GBDT对弱学习器的要求是要足够简单，即低方差&高偏差，一般深度不会超过5，叶子节点的数量也不会超过10，默认选择CART回归树。
   
    注意GBDT中的树都是回归树，通过改变损失函数（使用指数、对数损失函数）后也可以用于解决分类问题，但不是基学习器变成分类树。
    
    GBDT有三部分构成：DT（Regression Decision Tree）、GB（Gradient Boosting）和衰减（Shrinkage）。Shrinkage思想认为，每次走一小步逐渐逼近结果，
    要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，认为每棵树只学到了真理的一部分，所以累加的时候只累加一部分，通过多学几棵树弥补不足。
    
    回归树总体流程类似于分类树，区别在于损失函数不同，回归树节点的划分标准不再和熵有关，而是要最小化损失函数，一般为均方误差或者绝对值误差；而在分类算法中的损失函数一般选择对数函数。

XGBoost（eXtreme Gradient Boosting）是GBDT算法的一种变种，基学习器既可以是CART（gbtree），也可以是线性分类器（gblinear），
对传统的GBDT算法做了很多细节改进，包括使用损失函数二阶导数、用模型复杂度作为正则项、分裂点查找近似算法、稀疏感知算法、并行化训练设计等。
XGBoost是伸缩性强、便捷的可以并行构建模型的一种梯度提升算法。


xgboost模型 参数说明
params = {
    'booster':'gbtree',
    'objective':'multi:softmax',   # 多分类问题
    'num_class':10,  # 类别数，与multi softmax并用
    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子
    'max_depth':12,  # 构建树的深度，越大越容易过拟合
    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合
    'subsample':0.7, # 随机采样训练样本
    'colsample_bytree':3,# 这个参数默认为1，是每个叶子里面h的和至少是多少
    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1
    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，
    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合
    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0
    'eta':0.007,  # 如同学习率
    'seed':1000,
    'nthread':7,  #CPU线程数
    #'eval_metric':'auc'
}


参数调优的方法步骤一般情况如下：

(1) 选择较高的学习速率（learning rate）。一般情况下，学习速率的值为0.1。但是对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。
    选择对应于此学习速率的理想决策树数量。 Xgboost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。
    
(2 )对于给定的学习速率和决策树数量，进行决策树特定参数调优（max_depth，min_child_weight，gamma，subsample，colsample_bytree）。在确定一棵树的过程中，我们可以选择不同的参数。

(3) Xgboost的正则化参数的调优。（lambda，alpha）。这些参数可以降低模型的复杂度，从而提高模型的表现。

(4) 降低学习速率，确定理想参数。




