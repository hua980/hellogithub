
参考：https://blog.csdn.net/zwqjoy/article/details/80431496

集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。
集成学习的思路是通过合并多个模型来提升机器学习性能。

一般来说集成学习可以分为三大类：
用于减少方差的bagging
用于减少偏差的boosting
用于提升预测结果的stacking

1.Bagging
 
Bagging是引导聚合的意思。减少一个估计方差的一种方式就是对多个估计进行平均。例如，我们可以用训练集的不同子集（随机选择并替代训练集）训练M个不同的树然后计算最后的结果：

Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。

1）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping（有放回）的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（我们这里假设k个训练集之间是相互独立的，事实上不是完全独立）

2）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。但是是同种模型。（注：，k个训练集虽然有重合不完全独立，训练出来的模型因为是同种模型也是不完全独立。这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

3）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

对于Bagging需要注意的是，每次训练集可以取全部的特征进行训练，也可以随机选取部分特征训练，例如随机森林就是每次随机选取部分特征
bagging-----常用的集成算法模型是随机森林和随机树



2.Boosting(提高)

Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是利用加权的数据。在训练的早期对于错分数据给予较大的权重。

对于训练好的弱分类器，如果是分类任务按照权重进行投票，而对于回归任务进行加权，然后再进行预测。boosting和bagging的区别在于是对加权后的数据利用弱分类器依次进行训练。

boosting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：

先从初始训练集训练出一个基学习器；
再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注；
基于调整后的样本分布来训练下一个基学习器；
重复进行上述步骤，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。
最常用的一种boosting算法叫做AdaBoost，表示自适应boosting。

AdaBoost算法每一轮都要判断当前基学习器是否满足条件，一旦条件不满足，则当前学习器被抛弃，且学习过程停止。
由于属于boosting算法族，采用的是加性模型，对每个基学习器的输出结果加权处理，只会得到一个输出预测结果。所以标准的AdaBoost只适用于二分类任务。

3.Stacking（堆叠）
Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。









