
随机森林概念 参考：https://www.cnblogs.com/wj-1314/p/9628303.html

1.随机森林是一种有监督学习算法，是以决策树为基学习器的集成学习算法。

2.随机森林的构建过程
（1）从原始训练集中使用Bootstraping方法随机有放回采样取出m个样本，共进行n_tree次采样。生成n_tree个训练集

（2）对n_tree个训练集，我们分别训练n_tree个决策树模型

（3）对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数  选择最好的特征进行分裂

（4）每棵树都已知这样分裂下去，知道该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝

（5）将生成的多颗决策树组成随机森林。对于分类问题，按照多棵树分类器投票决定最终分类结果；对于回归问题，由多颗树预测值的均值决定最终预测结果


3.随机森林的随机性体现在哪几个方面？
（1）数据集的随机选取
　　从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

（2）待选特征的随机选取
　　与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征


4.随机森林的应用

特征重要性评估（特征选择）
（1）用随机森林进行特征重要性评估的思想就是看每个特征在随机森林中的每棵树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。
    贡献大小通常使用基尼指数（Gini index）或者袋外数据（OOB）错误率作为评估指标来衡量。这里我们再学习一下基尼指数来评价的方法。

（2） 特征选择的步骤
　　在特征重要性的基础上，特征选择的步骤如下：
    计算每个特征的重要性，并按降序排序
    确定要剔除的比例，依据特征重要性剔除相应比例的特征，得到一个新的特征集
    用新的特征集重复上述过程，直到剩下m个特征（m为提前设定的值）
    根据上述代码中得到的各个特征集合特征集对应的袋外误差率，选择袋外误差率最低的特征集
    
（3）特征重要性的估计方法
　　特征重要性的估计通常有两种方法：一是使用uniform或者gaussian抽取随机值替换原特征；一是通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布，
    第二种方法更加科学，保证了特征替代值与原特征的分布是近似的。这种方法叫做permutation test ，即在计算第i个特征的重要性的时候，将N 个特征的第i个特征重新洗牌，然后比较D和表现的差异性，如果差异很大，则表明第i个特征是重要的。

注意  OOB（out-of-bag ）：每棵决策树的生成都需要自助采样，这时就有1/3的数据未被选中，这部分数据就称为袋外数据。


5.调参步骤（利用网格搜索进行最佳参数选择）

（1）首先先调既不会增加模型复杂度，又对模型影响最大的参数n_estimators（子模型数），可以使用Grid SearchCV（网格搜索）探索n_estimators的最佳值

（2）找到最佳值后，调max_depth 最大树深度（单个网格搜索，也可以使用学习曲线）
　　一般根据数据的大小来进行一个探视，当数据集很小的时候，可以采用1~10，或者1~20这样的试探，但是对于大型数据来说，我们应该尝试30~50 层深度（或许更深）
  
（3）接下来依次对各个参数进行调参
　　注意：对大型数据集，max_leaf_nodes可以尝试从1000来构建，先输入1000，每100个叶子一个区间，再逐渐缩小范围
　　对于min_samples_split（最小样本数min_samples_split） 和 min_samples_leaf（叶子节点最小样本数min_samples_leaf）可用网格搜索，一般从他们的最小值开始向上增加10或者20，面对高纬度高样本数据，
    如果不放心可以直接50+，对于大型数据可能需要200~300的范围，如果调整的时候发现准确率无论如何都上不来，可以放心大胆的调试一个很大的数据，大力限制模型的复杂度。
    最大特征数max_features最佳参数的选取（网格搜索）






