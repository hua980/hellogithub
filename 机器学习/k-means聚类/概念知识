k-means聚类

1.k-means和knn的比较

  K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。
  KNN中喂给它的数据集是带label的数据，已经是完全正确的数据，而K-Means喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序
  KNN不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。
  两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻的思想。
  
  
  
2.k-means的原理

（1）在数据范围内以均匀分布随机生成与数据相同维度的k个点，作为初始的质心

（2）计算每一个样本数据与每一个质心的距离（可以是欧氏距离或曼哈顿距离等，但一般采用欧氏距离），并将其分组到距离最近的质心

（3）分组结束后，对于每一个分组，计算他们的中心（对分组每一个维度求均值，这也是算法名称的来由：均值means）

（4）更新质心的位置（移动到对应分组的中心）

以上 2，3，4步称为一个迭代，重复迭代，直到质心不移动或达到设定的最大的迭代次数



2.k值的选取和质心的选择

k值的选取---手肘法
手肘法的核心指标是SSE(sum of the squared errors，误差平方和)
手肘法的核心思想是：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。
并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，
所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

质心的选择
K-means++

随机初始化质心可能导致算法迭代很慢，K-means++是对K-mean随机初始化质心的一个优化，具体步骤如下：
随机选取一个点作为第一个聚类中心。
计算所有样本与第一个聚类中心的距离。
选择出上一步中距离最大的点作为第二个聚类中心。
迭代：计算所有点到与之最近的聚类中心的距离，选取最大距离的点作为新的聚类中心。
终止条件：直到选出了这k个中心。

只需要随机取第一个聚类中心即可。

然后按照最远优先原则来选新的聚类中心









