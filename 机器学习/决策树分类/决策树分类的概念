
1.什么是决策树
       决策树是一种树形结构，其中每个节点表示在一个属性上的测试，每个分支代表一个特征的测试结果，每个叶子节点代表一种类别。
       
 
2.决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。

  决策树的生成：根据特征选择标准，从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，并依次从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。


3.三种特征选择的方式：
参考：https://www.cnblogs.com/fengfenggirl/p/classsify_decision_tree.html

（1）信息增益（Information gain） --ID3算法

（2）增益比率（gain ratio）       --C4.5的生成算法

（3）基尼指数（Gini index）       --CART算法


4.决策树的剪枝

（1）预剪枝：边建立决策树边进行剪枝的操作（更实用）
     在决策树生成分支的过程，可以利用统计学的方法对即将分支的节点进行判断，比如统计信息增益，如果分支后使得子集的样本统计特性不满足规定的阈值，则停止分支
     
     剪枝规则：
     信息增益（率）少于阀值就不再进行分裂
     节点的样本个数少于阀值（比如1%）就不再分裂
     树的深度大于阀值（比如8）就不再分裂
     
（2）后剪枝：当建立完决策树后来进行剪枝操作
      根据每个分支的分类错误率及每个分支的权重，计算该节点不修剪时预期分类错误率；
      对于每个非叶节点，计算该节点被修剪后的分类错误率，如果修剪后分类错误率变大，即放弃修剪；否则将该节点强制为叶节点，并标记类别。
      
      从下往上依次判断：如果剪掉子树（把父节点作为叶子节点）能否让测试集的误差下降，如果可以，则进行剪枝
      
      
      
