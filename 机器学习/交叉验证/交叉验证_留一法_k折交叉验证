
针对经验风险最小化算法的过拟合的问题，给出交叉验证的方法，这个方法在做分类问题时很常用

交叉验证：
import sklearn.model_selection as ms
import numpy as np
# 使用给出的模型,针对输入与输出进行5次交叉验证
# 把每次交叉验证得到的精准度得分以数组的方式返回
# 再用np.average求均值

score = ms.cross_val_score(
    模型, 输入集, 输出集,
    cv=5,          	   # 交叉验证的次数
    scoring='accuracy' # 指标名称 (精准度)
)
np.average(score)

其中，scoring='accuracy' # 指标名称有：
精准度(accuracy): 分类正确的样本数/总样本数

查准率(precision_weighted): 针对每一类别, 预测正确的样本数 / 预测出来的样本数

召回率(recall_weighted): 针对每一类别, 预测正确的样本数 / 实际存在的样本数

f1得分(f1_weighted): 2x查准率x召回率/(查准率+召回率)


1 简单的交叉验证
(1) 从全部的训练数据 S中随机选择 中随机选择 s的样例作为训练集 train，剩余的 作为测试集 作为测试集 test。
(2) 通过对测试集训练 ，得到假设函数或者模型 。
(3) 在测试集对每一个样本根据假设函数或者模型，得到训练集的类标，求出分类正确率。
(4)选择具有最大分类率的模型或者假设。
这种方法称为 hold -out cross validation 或者称为简单交叉验证。由于测试集和训练集是分开的，就避免了过拟合的现象


2 k折交叉验证 k-fold cross validation
(1) 将全部训练集 S分成 k个不相交的子集，假设 S中的训练样例个数为 m，那么每一个子 集有 m/k 个训练样例，，相应的子集称作 {s1,s2,…,sk}。
(2) 每次从分好的子集中里面，拿出一个作为测试集，其它k-1个作为训练集
(3) 根据训练训练出模型或者假设函数。
(4) 把这个模型放到测试集上，得到分类率。
(5) 计算k次求得的分类率的平均值，作为该模型或者假设函数的真实分类率。

这个方法充分利用了所有样本。但计算比较繁琐，需要训练k次，测试k次。


3 留一法 leave-one-out cross validation
留一法就是每次只留下一个样本做测试集，其它样本做训练集，如果有k个样本，则需要训练k次，测试k次。

留一发计算最繁琐，但样本利用率最高。适合于小样本的情况。
